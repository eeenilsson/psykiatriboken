# Vetenskaplig metod

## Inledning

- Population
- Medelvärde
- Spridningsmått
- Konfidensintervall
- Prediktionsintervall

## Population

Figuren nedan visar ett fiktivt exempel på hur två olika studier representerar ett urval från hela populationen (grå färg). Medelvärde och spridning (konfidensintervall) är markerade med en punkt och en linje.

```{r } 
#| label: population 

## MADRS distribution parameters, from [@SchultevanMaaren2013] (4627 psychiatric outpatients):
madrs_distr <- c(11, 18, 23, 28, 36, 23.44, 7.75)
names(madrs_distr) <- c("P5", "P25", "P50", "P75", "P95", "Mean", "SD")
names(madrs_distr) <- tolower(names(madrs_distr))

## Generate population:
pacman::p_load(truncnorm)
pop = data.table(
    indx = 1:1000,
    madrs = round(rtruncnorm(n=1000, a=0, b=60, mean=madrs_distr[["mean"]], sd=madrs_distr[["sd"]]), digits=0),
    age = round(rtruncnorm(n=1000, a=18, b=65, mean=39.3, sd=12.3), digits=0),
    female = rbinom(n=1000, 1, prob= 0.616)
    ## sample = rbinom(n=1000, 2, prob= c(0.5, 0.25))
)

## Sample from pop to studies, weighting sampling by MADRS score:
set.seed(1)
pop[, study := 0]
indx_study1 <- sample(pop[, indx], 300, prob = pop[, madrs])
pop[indx %in% indx_study1, study := 1]
indx_study2 <- sample(pop[study !=1, indx], 100, prob = pop[study !=1, madrs^2])
pop[indx %in% indx_study2, study := 2]

## summary(pop)
## summary(pop[study==1, ])
## summary(pop[study==2, ])

## ## residuals:
## pop[, res := madrs - mean(madrs)]

## classes:
pop[, study:=factor(study)]

## Assign treatment and effect
set.seed(1)
pop[, trt := rbinom(n=1000, 1, prob= 0.5)]
pop[, effect_pbo := round(rtruncnorm(n=1000, a=-30, b=30, mean=-6, sd=9), digits=0)]
pop[, effect_trt := round(rtruncnorm(n=1000, a=-30, b=30, mean=-3, sd=9), digits=0)]
pop[, effect:=ifelse(trt==1, effect_trt+effect_pbo, effect_pbo)]
pop[, rest:=madrs+effect]
pop[, overshoot:=ifelse(rest<0, abs(rest), 0)]

pop[, effect_pbo:=effect_pbo+overshoot]
## pop[, effect_trt:=effect_trt+overshoot]

## re-calculate:
pop[, effect:=ifelse(trt==1, effect_trt+effect_pbo, effect_pbo)]
pop[, rest:=madrs+effect]
pop[, madrs_after:=madrs+effect]

## ## make a model:
## m <- lm(madrs_after~madrs+age+female+trt, data=pop[study==1, ])
## summary(m)
## confint(m)

## pop[trt==1, ]
## pop[, rest]

## pop[, madrs_after := round(rtruncnorm(n=1000, a=0, b=60, mean=madrs_distr[["mean"]], sd=madrs_distr[["sd"]]), digits=0)]

## For all patients (n = 1357) the observed mean treatment difference (escitalopram v. placebo) from baseline after 8 weeks of treatment (LOCF) was 3.2 (s.d. = 9.5) MADRS points, https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/assessing-the-true-effect-of-active-antidepressant-therapy-v-placebo-in-major-depressive-disorder-use-of-a-mixture-model/BF5EB5DC141E78C610F260300C2FFD79

## https://link.springer.com/article/10.1007/s00213-014-3584-4:
## Depressed patients assigned to placebo in trials using taped SIGMA interviews with RAPS appraisal had a significantly larger MADRS change score (M = −11.5 ± 12.7) compared to patients assigned to placebo in trials using traditional semi-structured interviews (−5.4 ± 8.9; F(df = 1.57) = 5.58, p = 0.022).

## Summary stats:
s <- pop[, .(n=.N, max = max(madrs), q99 = quantile(madrs, 0.99), m = mean(madrs), sd=sd(madrs), maxcount = max(density(madrs)$y*.N)), by = c("study")]
s <- rbind(
    s,
 pop[, .(study = NA, n=.N, max = max(madrs), q99 = quantile(madrs, 0.99), m = mean(madrs), sd=sd(madrs), maxcount = max(density(madrs)$y*.N))]
)

## SE and CI
s[, se := sd/sqrt(n)]
s[, ci_lower:=m-se*1.96]
s[, ci_upper:=m+se*1.96]
source("../functions/round_df.r")
s <- round_df(s, 2)
## is.data.table(s)
s[, lab := ifelse(!is.na(study), paste0("Study ", study, " M"), "Population M")]

## Plot:
p <- ggplot(pop, aes(madrs)) + geom_density(aes(y = after_stat(count)), alpha = 0.2)
p <- p + geom_density(aes(y = after_stat(count), fill = study), alpha = 0.2, data=pop[study==1|study==2, ])
use <- s[is.na(study)|study==1|study==2, ]
p <- p + geom_point(data = use, aes(y=maxcount-max(maxcount)*0.05, x=m), pch=2, size = 2)
p <- p + geom_segment(aes(x = ci_lower, y = maxcount-max(maxcount)*0.05, xend = ci_upper, yend = maxcount-max(maxcount)*0.05), arrow = arrow(angle=90, ends="both", length = unit(0.25, "cm")), data = use)
p_density <- p

## Plot 2
pop2 <- pop
a <- pop[study==1|study==2, ]
a[, study:=NA]
pop2[study==0, study:=NA]
pop2 <- rbind(pop2, a) ## add study 1 and 2 with study = NA

p <- ggplot(pop2, aes(madrs, y = study, color=study)) +
    geom_point(position=position_jitter(width=0.2, height=0.2)) +
    labs(x="MADRS", y="Studie")

p_strip <- p+geom_pointrange(aes(y=study, x=m, xmin=ci_lower, xmax=ci_upper), data=s[is.na(study)|study==1|study==2, ], fatten=8, linewidth=1, colour="black")

## VAriance
## The formula for variance (s2) is the sum of the squared differences between each data point and the mean, divided by the number of data points. When working with data from a complete population the sum of the squared differences between each data point and the mean is divided by the size of the data set, n. When working with a sample, divide by the size of the data set minus 1, n - 1. Take the square root of the sample variance to get the standard deviation.

## MADRS
## Items are rated on a 7-point Likert scale anchored at 4 points (0: symptom is absent; 6: symptom is totally dominant) and summed to yield a total score that ranges from 0 to 60. Scores from 0 to 8 denote “Normal/no” depression; total scores from 9 to 18 denote “Possible/mild” depression; total scores from 19 to 26 denote “Moderate” depression; total scores from 27 to 34 denote “Severe” depression; and scores from 35 onward denote “Very severe” depression

## pARTICIPANTS 
## The ROM patient-group consisted of a baseline sample of 4627 psychiatric outpatients, aged between 18 and 65 years (61.0% females; mean age=39.3, SD=12.3), diagnosed with and treated for depressive disorders (MDD or dysthymia) in the Leiden University Medical Center (LUMC) Department of Psychiatry or the Rivierduinen specialized mental healthcare centers. Baseline assessment was part of the usual ROM procedure. On average, 80% of the referred patients with a tentative diagnosis of Mood-, Anxiety- and/or Somatoform (MAS) disorder were assessed with ROM in the study period



``` 

```{r } 
#| label: p-strip
p_strip
``` 

Referens MADRS distribution se [@SchultevanMaaren2013].

## Beräkning av varians och precision

- Residual = observerat värde - medelvärde
- Varians (S^2) = summa(residualer^2) / n-1
- Standardavvikelse (SD) = sqrt(varians)
- Standard Error (SE) = SD/sqrt(n)
- Konfidensintervall = Medelvärde +/- 1.96*SE

## Effekt

a. Sann effekt $\theta$, eller i en viss studie $\theta_\kappa$
b. Observerad effekt $\hat{\theta}_\kappa$

a och b skiljer sig pga att urvalet representerar ej hela populationen och därför ger upphov till sampling error $\epsilon_\kappa$

\begin{equation}
\hat{\theta}_\kappa$ = $\hat{\theta} + \epsilon_\kappa
\end{equation}

$\epsilon_\kappa$ kan kvantifieras av Standard Error (SE), vilket bygger på ett antagande om fördelning (oftast normalfördelning)

<!-- \begin{displaymath}SE=s/\sqrt{n}\end{displaymath} -->

\begin{equation}
SE = \frac{s}{\sqrt{n}}
\end{equation}

s är standardavvikelsen

<!-- $\begin{math} SE \end{math}$  -->

## Medelvärde mm

Det aritmetiska medelvärdet $\bar{x}$ beräknas genom att summera alla individuella värden $x_i$ i urvalet och sedan dividera summan med urvalets storlek (n).

\begin{equation}
\bar{x} = \frac{\sum^{n}_{i=1}x_i}{n}
\end{equation}

Standard error of a proportion:

\begin{equation}
SE_{p} = \sqrt{\frac{p(1-p)}{n}}
\end{equation}

Proportioner transformeras ofta med logit innan de slås samman (annars skevhet om p nära 0 eller 1). För detta används odds.

## RR och OR

| Exposed | Outcome | Not Outcome | Risk              | Odds        |
|---------|---------|-------------|-------------------|-------------|
| Yes     | A       | B           | A/(A+B)           | A/B         |
| No      | C       | D           | C/(C+D)           | C/D         |
| Ratio   |         |             | (A/(A+B))/C/(C+D) | (A/B)/(C/D) |

Risk: Cumulative incidence (average risk). Om utfallet är ovanligt är odds~risk.

<!-- [@Cummings2009] -->

<!-- CMH: https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704-ep713_confounding-em/BS704-EP713_Confounding-EM7.html -->

<!-- The Upper Limits of Risk Ratios and Recommendations for Reporting Risk Ratios, Odds Ratios, and Rate Ratios[@Chao] -->

<!-- Zero counts add 0.5? (Behövs ej vid MH): https://handbook-5-1.cochrane.org/chapter_16/16_9_2_studies_with_zero_cell_counts.htm -->


## Prediktionsintervall och konfidensintervall

Exempel: Låt säga att kroppslängden hos alla vuxna i världen är i medeltal 171 cm. I en studie av kroppslängd kommer ett urval av dessa att finnas med och medelvärdet kan då bli tex 168 cm. Det 95%-iga konfidensintervallet i studien kan vara tex 160-175 cm (hur snävt det blir beror på studiens N). Siffran 95% innebär att om studien görs om flera gånger kommer 95% av studiernas CI att innehålla det sanna medelvärdet (171 cm).

Konfidensintervallet representerar precisionen för det beräknade medelvärdet. Man kan säga att CI är det intervall där poulationens sanna medelvärde sannolikt ligger.

Prediktionsintervall (PI) är en uppskattning av det intervall inom vilket en enda framtida observation (med en viss sannolikhet) kommer att falla, givet vad som redan observerats. Det uttalar sig alltså inte om medelvärdet.

Exempel: Om man mäter ett antal kast med en 6-sidig tärning kommer konfidensintervallet att krympa närmare medelvärdet 3.5 ju fler observationer som görs. Prediktionsintervallet däremot kommer att vara ca 1-6 oavsett hur många mätningar som gjorts.

I metaanlyser anges vanligen I2 och T2 som mått på heterogenitet. I2 är beroende av sample size (K), vilket kan ge låga (icke signifikanta) värden när det är få studier och tvärtom. T2 är svårtolkat eftersom det är ett värde upphöjt i två och därför inte korresponderar till effektmåttet[@FaggionJr2021].

I metaanalyser är prediktionintervallet användbart eftersom det fångar heterogenitet mellan studier på samma skala som utfallet har mätts med[@FaggionJr2021] (i motsats till andra vanliga mått på heterogenitet).

I en metaanlys representerar PI variabiliteten i effekten över olika "settings" (studier med tex olika inklusionskriterier). När det är låg heterogenitet mellan studier närmar sig PI konfidensintervallet. Vid hög heterogenitet blir PI vidare än CI.

I en metaanalys är PI det intervall inom vilket medelvärdet i en ny studie förväntas falla om den studien valdes slumpmässigt från den population av studier som ingår i metaanalysen. Det kan också tolkas som en sammanfattning av spridningen av undeliggande effekter i de inkluderade studierna.

Problem med PI i metaanalyser är att det bygger på ett antagande om att effekterna i de olika studierna är normalfördelade, vilket ifall antalet studier är litet kan ge felaktigt snnävt eller stort PI. Cochrane rekommenderar att PI presenteras om det är >10 studier och en funnel plot inte visar någon tydlig assymetri.

<!-- When the pattern of the forest plot itself suggests that there are different types of population with rather different effect sizes (as in this example), then the “combined” effect size is not a useful parameter any more. [https://www.erim.eur.nl/fileadmin/erim_content/images/meta-essentials/How_to_interpret_results_of_meta-analysis_1.4.pdf] -->

<!-- I2 is a measure for the proportion of observed variance that reflects real differences in effect size. (See Borenstein et al., 2009: 117-119, for how I2 is computed.) It is expressed as a percentage with a range from 0 to 100 percent. It is a relative measure. It is not a measure of variation in terms of the scale of the effect size parameter. Hence its usefulness is limited. Borenstein et al. (2009: 119) advise to use I2 as a criterion for a decision whether a subgroup analysis or moderator analysis (discussed below) is indicated. If I2 is low, then there is no heterogeneity to speak of and hence nothing to be explored in a subgroup or moderator analysis. If I2 is large, then such an analysis is likely to be worthwhile. -->

<!-- Both T2 and Tau are measures of the dispersion of true effect sizes between studies in terms of the scale of the effect size. (See Borenstein et al., 2009: 114-117, for how T2 and Tau are estimated.) T2 is an estimate of the variance of the true effect sizes. Or, in the words of Borenstein et al. (2009: 114): “If we had an infinitely large sample of studies, each itself infinitely large (so that the estimate in each study was the true effect) and computed the variance of these effects, this variance would be T2”. T2 is not used itself as a measure of heterogeneity but is used in two other ways: (1) it is used to compute Tau; and (2) it is used to assign weights to the studies in the meta-analysis under the random-effects model (discussed below). Tau is an estimate of the standard deviation of the distribution of true effect sizes, under the assumption that these true effect sizes are normally distributed. Tau is used for computing the prediction interval. -->

<!-- Summarizing, how should this multiple information about heterogeneity be interpreted and used? We recommend to use I2 as the main source of information about the extent of heterogeneity. As soon as I2 is larger than an (arbitrary) proportion (say 25%), the meta-analyst should not interpret the combined effect size as meaningful and should not conduct any form of significance testing. After such a decision has been made, the meta-analyst should focus on an analysis of the dispersion of true effect sizes, and of its determinants (moderators). Tau is a useful first indication of the extent of this dispersion.  However, the prediction interval is a more direct and more easily interpretable indicator. -->

<!-- Because the prediction interval is estimated based on the effect sizes observed in the studies that are meta- analysed, the prediction interval corresponds more or less (depending on whether sampling variation in the meta-analysed studies is large or small) with the range of effect sizes that are meta-analysed and that are represented in the forest plot. This implies that the prediction interval can only “predict” with some accuracy if no relevant selection bias exists in the set of populations that have been studied (i.e., if the populations of which the effect size estimate is included in the meta-analysis are “representative” for the domain). If selection bias exists, then effect sizes observed in future studies might occur beyond the limits of the prediction interval. Because selection bias is more likely than not (which will be argued below), it is recommended to interpret the prediction interval as a description of the range of observed effect sizes rather than as a prediction of the range of effect sizes that will be observed in future studies (despite its name “prediction” interval). -->

Probabilistiskt svar på AD: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8134509/

## Resurser

[Cohcrane](https://www.cochranelibrary.com/search)

[Cochrane handbook](https://training.cochrane.org/handbook/current/chapter-10)

[Metaanalyser i R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/effects.html)
